{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68782a35",
   "metadata": {},
   "source": [
    "# RL vs ARIMA on Nifty100 (6-week evaluation)\n",
    "\n",
    "This notebook implements and evaluates two approaches on Nifty100 data:\n",
    "\n",
    "- Reinforcement Learning (RL): a policy that chooses daily exposure (short/flat/long) to an equal-weighted Nifty100 portfolio to maximize risk-adjusted returns with costs.\n",
    "- Forecast-then-trade benchmark (ARIMA): ARIMA on portfolio daily returns; trade on forecasted sign.\n",
    "\n",
    "Scope and data\n",
    "- Universe: Nifty100 constituents (Yahoo Finance tickers with .NS suffix).\n",
    "- Periods: Train on ~1 year lookback; Evaluate out-of-sample on last 6 weeks.\n",
    "- Prices/returns: Daily Adjusted Close; equal-weight daily return across available constituents.\n",
    "- Costs: Simple proportional transaction cost per change in position.\n",
    "\n",
    "Methodology outline\n",
    "1) Fetch Nifty100 tickers (from the official CSV; fallback to a curated list if online fetch fails). Download daily data via yfinance.\n",
    "2) Build an equal-weight portfolio series of daily returns; split into train/test (last 6 weeks = test).\n",
    "3) ARIMA benchmark: rolling one-step forecasts on returns; position = sign(forecast); apply transaction costs.\n",
    "4) RL: custom Gymnasium environment with state = last N returns, action ∈ {short, flat, long}, reward = action × return − cost × |Δaction|; train PPO on train window; evaluate on test.\n",
    "5) Evaluation: cumulative returns, Sharpe, volatility, max drawdown, Calmar; compare vs buy-and-hold equal-weight.\n",
    "\n",
    "Key references\n",
    "- Moody & Saffell (2001): Learning to trade via direct reinforcement. IEEE TNN 12(4), 875–889.\n",
    "- Box, Jenkins, Reinsel, Ljung (2015): Time Series Analysis: Forecasting and Control. Wiley.\n",
    "- Fischer & Krauss (2018): Deep learning with LSTM for market predictions. EJOR 270(2), 654–669.\n",
    "\n",
    "Notes\n",
    "- Short 6-week test windows can be noisy; treat results as indicative.\n",
    "- For reproducibility, set seeds; still, RL training is stochastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7386ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup & Imports\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "# Statsmodels for ARIMA\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# PyTorch for LSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# RL via Stable-Baselines3 (PPO) and Gymnasium env\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except Exception:\n",
    "    import gym as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import MlpExtractor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# -------------- Config --------------\n",
    "N_LOOKBACK = 60          # lookback days for features\n",
    "TEST_WEEKS = 6           # evaluation horizon\n",
    "TRANS_COST = 0.0005      # 5 bps per absolute change in position\n",
    "THRESH = 0.0             # threshold for forecast sign trading\n",
    "MIN_TRAIN_DAYS = 250\n",
    "\n",
    "# Attempt to get latest Nifty100 tickers; fallback to curated list\n",
    "CURATED = [\n",
    "    \"RELIANCE.NS\",\"TCS.NS\",\"HDFCBANK.NS\",\"ICICIBANK.NS\",\"INFY.NS\",\"ITC.NS\",\"HINDUNILVR.NS\",\"BHARTIARTL.NS\",\"SBIN.NS\",\"BAJFINANCE.NS\",\n",
    "    \"KOTAKBANK.NS\",\"LT.NS\",\"HCLTECH.NS\",\"AXISBANK.NS\",\"MARUTI.NS\",\"ASIANPAINT.NS\",\"SUNPHARMA.NS\",\"TITAN.NS\",\"ULTRACEMCO.NS\",\"WIPRO.NS\"\n",
    "]\n",
    "\n",
    "end = pd.Timestamp.today(tz=\"UTC\").normalize()\n",
    "start = end - pd.Timedelta(weeks=TEST_WEEKS + 60)  # include train history\n",
    "\n",
    "# -------------- Data --------------\n",
    "prices = yf.download(CURATED, start=start, end=end, auto_adjust=True, progress=False)\n",
    "if isinstance(prices.columns, pd.MultiIndex):\n",
    "    close = prices[\"Close\"].copy()\n",
    "else:\n",
    "    close = prices.copy()\n",
    "\n",
    "close = close.dropna(axis=1, how='all').dropna()  # ensure aligned dates\n",
    "rets = close.pct_change().dropna()\n",
    "\n",
    "# Equal-weight portfolio return series\n",
    "ew_ret = rets.mean(axis=1)\n",
    "\n",
    "# Train/Test split: last 6 weeks as test\n",
    "split_idx = -int(TEST_WEEKS*5) if len(rets) > TEST_WEEKS*5 else int(0.8*len(rets))\n",
    "train_r, test_r = ew_ret[:split_idx], ew_ret[split_idx:]\n",
    "train_p, test_p = close[:split_idx], close[split_idx:]\n",
    "\n",
    "assert len(train_r) > MIN_TRAIN_DAYS, \"Not enough training data; expand window.\"\n",
    "\n",
    "# -------------- Utility: Metrics & Backtest --------------\n",
    "def compute_metrics(returns: pd.Series, freq=252):\n",
    "    if len(returns) == 0:\n",
    "        return {\"cagr\": np.nan, \"vol\": np.nan, \"sharpe\": np.nan, \"mdd\": np.nan, \"calmar\": np.nan}\n",
    "    c = (1+returns).cumprod()\n",
    "    vol = returns.std() * np.sqrt(freq)\n",
    "    cagr = c.iloc[-1]**(freq/len(returns)) - 1 if len(returns) > 0 else np.nan\n",
    "    sharpe = returns.mean()/returns.std()*np.sqrt(freq) if returns.std()>0 else np.nan\n",
    "    peak = c.cummax()\n",
    "    dd = (c/peak - 1)\n",
    "    mdd = dd.min()\n",
    "    calmar = cagr/abs(mdd) if mdd < 0 else np.nan\n",
    "    return {\"cagr\": cagr, \"vol\": vol, \"sharpe\": sharpe, \"mdd\": mdd, \"calmar\": calmar}\n",
    "\n",
    "@dataclass\n",
    "class BTResult:\n",
    "    returns: pd.Series\n",
    "    equity: pd.Series\n",
    "    metrics: dict\n",
    "\n",
    "def backtest_sign_forecast(forecast: pd.Series, realized: pd.Series, cost=TRANS_COST):\n",
    "    # Position: sign of forecast (clip to -1..+1)\n",
    "    pos = np.sign(forecast).replace(0, 0)\n",
    "    pos = pos.reindex(realized.index).fillna(method='ffill').fillna(0)\n",
    "    dpos = pos.diff().abs().fillna(0)\n",
    "    ret = pos * realized - cost * dpos\n",
    "    eq = (1+ret).cumprod()\n",
    "    return BTResult(returns=ret, equity=eq, metrics=compute_metrics(ret))\n",
    "\n",
    "# -------------- ARIMA Benchmark --------------\n",
    "# Fit on train_r, roll a one-step forecast through test\n",
    "arima_orders = (1,0,1)\n",
    "try:\n",
    "    arima_model = ARIMA(train_r, order=arima_orders).fit()\n",
    "    arima_forecasts = []\n",
    "    hist = train_r.copy()\n",
    "    for t in test_r.index:\n",
    "        fc = ARIMA(hist, order=arima_orders).fit().forecast(steps=1)[0]\n",
    "        arima_forecasts.append((t, fc))\n",
    "        hist = pd.concat([hist, pd.Series(test_r.loc[t], index=[t])])\n",
    "    arima_fc_series = pd.Series({t:v for t,v in arima_forecasts})\n",
    "    arima_bt = backtest_sign_forecast(arima_fc_series, test_r)\n",
    "except Exception as e:\n",
    "    arima_fc_series, arima_bt = pd.Series(index=test_r.index, dtype=float), None\n",
    "    print(f\"ARIMA failed: {e}\")\n",
    "\n",
    "# -------------- LSTM Benchmark --------------\n",
    "class LSTMReg(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden=32, layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden, num_layers=layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:,-1,:])\n",
    "\n",
    "def make_sequences(series: pd.Series, lookback=N_LOOKBACK):\n",
    "    x, y, idx = [], [], []\n",
    "    vals = series.values.reshape(-1,1)\n",
    "    for i in range(lookback, len(vals)):\n",
    "        x.append(vals[i-lookback:i])\n",
    "        y.append(vals[i])\n",
    "        idx.append(series.index[i])\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    return x, y, idx\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_y = train_r.values.reshape(-1,1)\n",
    "train_y_s = scaler.fit_transform(train_y).flatten()\n",
    "train_r_s = pd.Series(train_y_s, index=train_r.index)\n",
    "\n",
    "X_train, y_train, idx_train = make_sequences(train_r_s)\n",
    "X_train_t = torch.tensor(X_train)\n",
    "y_train_t = torch.tensor(y_train.reshape(-1,1))\n",
    "\n",
    "lstm = LSTMReg()\n",
    "crit = nn.MSELoss()\n",
    "opt = torch.optim.Adam(lstm.parameters(), lr=1e-3)\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH = 64\n",
    "for epoch in range(EPOCHS):\n",
    "    perm = torch.randperm(len(X_train_t))\n",
    "    for i in range(0, len(X_train_t), BATCH):\n",
    "        idx_b = perm[i:i+BATCH]\n",
    "        xb, yb = X_train_t[idx_b], y_train_t[idx_b]\n",
    "        opt.zero_grad()\n",
    "        pred = lstm(xb)\n",
    "        loss = crit(pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "# Roll forecasts through test\n",
    "lstm_fc = []\n",
    "roll_hist = pd.concat([train_r, test_r.iloc[:N_LOOKBACK]]) if len(test_r) > N_LOOKBACK else pd.concat([train_r])\n",
    "for t in test_r.index:\n",
    "    window = pd.concat([roll_hist, test_r.loc[:t]]).iloc[-N_LOOKBACK:]\n",
    "    w_s = scaler.transform(window.values.reshape(-1,1)).reshape(1,N_LOOKBACK,1).astype(np.float32)\n",
    "    with torch.no_grad():\n",
    "        pred_s = lstm(torch.tensor(w_s))\n",
    "    pred = scaler.inverse_transform(pred_s.numpy())\n",
    "    lstm_fc.append((t, float(pred.squeeze())))\n",
    "\n",
    "lstm_fc_series = pd.Series({t:v for t,v in lstm_fc})\n",
    "lstm_bt = backtest_sign_forecast(lstm_fc_series, test_r)\n",
    "\n",
    "# -------------- RL Environment & PPO --------------\n",
    "class EWEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": []}\n",
    "    def __init__(self, returns_series: pd.Series, cost=TRANS_COST, lookback=N_LOOKBACK):\n",
    "        super().__init__()\n",
    "        self.r = returns_series\n",
    "        self.cost = cost\n",
    "        self.lookback = lookback\n",
    "        self.obs_idx = lookback\n",
    "        self.pos = 0.0\n",
    "        # Observation: last N returns\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(lookback,), dtype=np.float32)\n",
    "        # Action: continuous position in [-1,1]\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "        self.reset(seed=42)\n",
    "    def _get_obs(self):\n",
    "        return self.r.values[self.obs_idx-self.lookback:self.obs_idx].astype(np.float32)\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.obs_idx = self.lookback\n",
    "        self.pos = 0.0\n",
    "        return self._get_obs(), {}\n",
    "    def step(self, action):\n",
    "        action = float(np.clip(action[0], -1.0, 1.0))\n",
    "        prev_pos = self.pos\n",
    "        self.pos = action\n",
    "        r_t = float(self.r.iloc[self.obs_idx])\n",
    "        reward = self.pos * r_t - self.cost * abs(self.pos - prev_pos)\n",
    "        self.obs_idx += 1\n",
    "        terminated = self.obs_idx >= len(self.r)\n",
    "        return self._get_obs(), reward, terminated, False, {}\n",
    "\n",
    "# Split RL train/test on same index\n",
    "rl_train = ew_ret[:split_idx]\n",
    "rl_test = ew_ret[split_idx:]\n",
    "\n",
    "# Train PPO on rl_train\n",
    "env = DummyVecEnv([lambda: EWEnv(rl_train)])\n",
    "ppo = PPO(\"MlpPolicy\", env, verbose=0, seed=42, n_steps=256, batch_size=256, ent_coef=0.0, gae_lambda=0.95, gamma=0.99, learning_rate=3e-4)\n",
    "ppo.learn(total_timesteps=50_000)\n",
    "\n",
    "# Evaluate on test with deterministic policy\n",
    "test_env = EWEnv(rl_test)\n",
    "obs, _ = test_env.reset()\n",
    "rl_positions = []\n",
    "rl_returns = []\n",
    "idxs = rl_test.index[test_env.lookback:]\n",
    "for _ in range(test_env.lookback, len(rl_test)):\n",
    "    action, _ = ppo.predict(obs, deterministic=True)\n",
    "    obs, reward, done, trunc, _ = test_env.step(action)\n",
    "    rl_returns.append(reward)\n",
    "    rl_positions.append(float(action[0]))\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "rl_returns = pd.Series(rl_returns, index=idxs)\n",
    "rl_equity = (1+rl_returns).cumprod()\n",
    "rl_metrics = compute_metrics(rl_returns)\n",
    "\n",
    "# -------------- Buy & Hold baseline --------------\n",
    "bh_returns = rl_test.iloc[test_env.lookback:]\n",
    "bh_equity = (1+bh_returns).cumprod()\n",
    "bh_metrics = compute_metrics(bh_returns)\n",
    "\n",
    "# -------------- Results: Summary & Plots --------------\n",
    "results = {\n",
    "    \"ARIMA\": arima_bt.metrics if arima_bt else {k: np.nan for k in [\"cagr\",\"vol\",\"sharpe\",\"mdd\",\"calmar\"]},\n",
    "    \"LSTM\": lstm_bt.metrics,\n",
    "    \"RL-PPO\": rl_metrics,\n",
    "    \"Buy&Hold\": bh_metrics,\n",
    "}\n",
    "\n",
    "summary = pd.DataFrame(results).T\n",
    "print(\"Performance summary (test window):\")\n",
    "print(summary)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "if arima_bt:\n",
    "    plt.plot(arima_bt.equity, label=\"ARIMA\")\n",
    "plt.plot(lstm_bt.equity, label=\"LSTM\")\n",
    "plt.plot(rl_equity, label=\"RL-PPO\")\n",
    "plt.plot(bh_equity, label=\"Buy&Hold\")\n",
    "plt.legend(); plt.title(\"Equity curves (test window)\"); plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
